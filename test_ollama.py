#!/usr/bin/env python3
"""
Test Ollama connection and setup
"""

import requests
import json

def test_ollama_connection():
    """Test if Ollama is running and accessible"""
    try:
        print("ğŸ§ª Testing Ollama connection...")
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            print("âœ… Ollama is running and accessible!")
            print(f"ğŸ“Š Found {len(models)} models:")
            
            if models:
                for model in models:
                    print(f"   - {model['name']} ({model.get('size', 'unknown size')})")
            else:
                print("   âš ï¸  No models found. You need to download a model:")
                print("   Run: ollama pull llama3.2")
            
            return True, models
            
        else:
            print(f"âŒ Ollama responded with status {response.status_code}")
            return False, []
            
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to Ollama. Is it running?")
        print("ğŸ’¡ Try: ollama serve")
        return False, []
        
    except Exception as e:
        print(f"âŒ Error testing Ollama: {e}")
        return False, []

def test_model_generation():
    """Test generating content with Ollama"""
    try:
        print("\nğŸ¤– Testing model generation...")
        
        payload = {
            "model": "llama3.2",  # or whatever model is available
            "prompt": "Generate a simple test case for user login functionality. Respond with just the test title.",
            "stream": False
        }
        
        response = requests.post("http://localhost:11434/api/generate", 
                               json=payload, 
                               timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            content = result.get('response', '')
            print("âœ… Model generation test successful!")
            print(f"ğŸ“ Sample response: {content[:100]}...")
            return True
        else:
            print(f"âŒ Generation failed with status {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Generation test failed: {e}")
        return False

def main():
    print("ğŸ§ª Ollama Connection Tester")
    print("=" * 40)
    
    # Test connection
    is_connected, models = test_ollama_connection()
    
    if not is_connected:
        print("\nğŸš€ Next steps:")
        print("1. Start Ollama: ollama serve")
        print("2. Download a model: ollama pull llama3.2")
        print("3. Run this test again")
        return
    
    if not models:
        print("\nğŸ“¥ You need to download a model:")
        print("Run: ollama pull llama3.2")
        print("Then run this test again")
        return
    
    # Test generation
    if test_model_generation():
        print("\nğŸ‰ Ollama is ready for test case generation!")
        print("\nâœ… You can now run:")
        print("   streamlit run streamlit_app.py")
        print("   (Select 'ollama' as the AI provider)")
    else:
        print("\nâš ï¸  Ollama is connected but model generation failed")
        print("Try downloading a different model: ollama pull llama3.2")

if __name__ == "__main__":
    main()
